{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPxWyKo6SiOQ7xnYP31e6cI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jennykon1715/Crawling-data_ptt-travel/blob/main/ptt_travel_crawler.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GsxfOFGxYXL",
        "outputId": "c0a45900-32f6-4923-801e-edd30ff672a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "資料已經成功儲存成json\n"
          ]
        }
      ],
      "source": [
        "#取得ptt 網頁原始碼\n",
        "import json\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "#import json\n",
        "import pandas as pd\n",
        "\n",
        "url = \"https://www.ptt.cc/bbs/Japan_Travel/index.html\"\n",
        "\n",
        "#模仿使用者發出一個請求到伺服器端獲得網頁(加入user agent 才不會被認為是機器人，被反爬蟲踢掉)\n",
        "headers = {'User-Agent': 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Mobile Safari/537.36'} #先宣告一個字典\n",
        "response = requests.get(url, headers=headers) #前面的headers是關鍵字，可帶上所有的headers, 後面的headers是我們宣告的字典\n",
        "soup = BeautifulSoup(response.text, 'html.parser') #把response 丟到beautiful soup解析, parser=解析器\n",
        "\n",
        "#找到文章列表\n",
        "articles = soup.find_all(\"div\", class_='r-ent') #找到所有符合條件的元素，所以會出現列表\n",
        "#print(articles[0])\n",
        "\n",
        "#如何把它變成可以重複使用的資料? ->轉換成json格式\n",
        "#此列表用來儲存文章一筆一筆的資料\n",
        "data_list = []\n",
        "\n",
        "#使用for 迴圈跌代articles\n",
        "for a in articles:\n",
        "    data = {}\n",
        "    title = a.find('div', class_='title')\n",
        "    if title and title.a:\n",
        "        title = title.a.text\n",
        "    else:\n",
        "        title = '沒有標題'\n",
        "    data[\"標題\"] = title\n",
        "    #print(title)\n",
        "\n",
        "    popular = a.find('div', class_='nrec')\n",
        "    if popular and popular.span:\n",
        "        popular = popular.span.text\n",
        "    else:\n",
        "        popular = 'N/A'\n",
        "    data[\"人氣\"] = popular\n",
        "\n",
        "    date = a.find('div', class_='date')\n",
        "    if date:\n",
        "        date = date.text\n",
        "    else:\n",
        "        date = \"N/A\"\n",
        "    data['日期'] = date\n",
        "    data_list.append(data)\n",
        "df = pd.DataFrame(data_list) #將列表轉成dataframe\n",
        "df.to_excel(\"ppt_travel.xlsx\", index=False, engine=\"openpyxl\")\n",
        "\n",
        "#寫入檔案 jason\n",
        "with open('ptt_travel_data.json', 'w', encoding='utf-8') as file:\n",
        "    json.dump(data_list, file, ensure_ascii=False, indent=4)\n",
        "print('資料已經成功儲存成json')\n",
        "\n",
        "#print(data_list)\n",
        "\n",
        "\n",
        "    #print(f\"標題:{title} , 人氣:{popular} ,  DATE:{date}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#print(response.text) #將物件印出\n",
        "\n",
        "#如果網址正確，輸出寫入成功，網址錯誤-沒有抓到網頁\n",
        "#if response.status_code == 200 :\n",
        "   # with open('output.html', 'w', encoding='utf-8') as f:  # 存成html檔觀看,寫入檔案, w= write寫入模式)\n",
        "    #    f.write(response.text)\n",
        "    #print('寫入成功!')\n",
        "#else:\n",
        "    #print('沒有抓到網頁')\n",
        "\n",
        "#使用beautiful soup來解析此網頁\n",
        "\n",
        "\n"
      ]
    }
  ]
}